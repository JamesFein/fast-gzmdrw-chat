#!/usr/bin/env python3
"""
LlamaIndexé…ç½®æµ‹è¯•è„šæœ¬
æµ‹è¯•LlamaIndexæ˜¯å¦æ­£ç¡®ä½¿ç”¨base_urlé…ç½®
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from backend.config import settings
from llama_index.core import Settings
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI

def test_llamaindex_config():
    """æµ‹è¯•LlamaIndexé…ç½®"""
    print("ğŸ” æµ‹è¯•LlamaIndexé…ç½®...")
    
    print(f"ğŸ“– ä»é…ç½®æ–‡ä»¶è¯»å–:")
    print(f"   API Key: {settings.openai_api_key[:10]}...{settings.openai_api_key[-4:]}")
    print(f"   Base URL: {settings.openai_base_url}")
    print(f"   Model: {settings.openai_model}")
    print(f"   Embedding Model: {settings.embedding_model}")
    
    # é…ç½®LlamaIndex
    print("\nğŸ”§ é…ç½®LlamaIndex...")
    
    # è®¾ç½®åµŒå…¥æ¨¡å‹
    embed_model = OpenAIEmbedding(
        api_key=settings.openai_api_key,
        base_url=settings.openai_base_url,
        model=settings.embedding_model
    )
    
    # è®¾ç½®LLM
    llm = OpenAI(
        api_key=settings.openai_api_key,
        base_url=settings.openai_base_url,
        model=settings.openai_model,
        temperature=0.1
    )
    
    # è®¾ç½®å…¨å±€é…ç½®
    Settings.embed_model = embed_model
    Settings.llm = llm
    
    print("âœ… LlamaIndexé…ç½®å®Œæˆ")
    
    # æµ‹è¯•åµŒå…¥æ¨¡å‹
    print("\nğŸ” æµ‹è¯•åµŒå…¥æ¨¡å‹...")
    try:
        embeddings = embed_model.get_text_embedding("æµ‹è¯•æ–‡æœ¬")
        print(f"âœ… åµŒå…¥æ¨¡å‹æµ‹è¯•æˆåŠŸ")
        print(f"   å‘é‡ç»´åº¦: {len(embeddings)}")
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•LLM
    print("\nğŸ” æµ‹è¯•LLM...")
    try:
        response = llm.complete("è¯·å›å¤'æµ‹è¯•æˆåŠŸ'")
        print(f"âœ… LLMæµ‹è¯•æˆåŠŸ")
        print(f"   å›å¤: {response.text}")
    except Exception as e:
        print(f"âŒ LLMæµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                 LlamaIndex é…ç½®æµ‹è¯•å·¥å…·                      â•‘")
    print("â•‘                                                              â•‘")
    print("â•‘  ğŸ”§ æµ‹è¯•LlamaIndexé…ç½®                                      â•‘")
    print("â•‘  ğŸŒ éªŒè¯base_urlè®¾ç½®                                       â•‘")
    print("â•‘  ğŸ¤– æµ‹è¯•æ¨¡å‹å¯ç”¨æ€§                                         â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    success = test_llamaindex_config()
    
    print("\n" + "="*60)
    if success:
        print("ğŸ‰ LlamaIndexé…ç½®æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âŒ LlamaIndexé…ç½®æµ‹è¯•å¤±è´¥ï¼")
        print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("   1. æ£€æŸ¥LlamaIndexç‰ˆæœ¬æ˜¯å¦æ”¯æŒbase_urlå‚æ•°")
        print("   2. å°è¯•è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_BASE_URL")
        print("   3. æ£€æŸ¥APIå¯†é’¥å’Œä»£ç†åœ°å€æ˜¯å¦æ­£ç¡®")
    print("="*60)

if __name__ == "__main__":
    main()
