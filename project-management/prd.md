# æç®€ RAG èŠå¤©åº”ç”¨å¼€å‘è„šæœ¬

## é¡¹ç›®æ¦‚è¿°

**ç›®æ ‡**ï¼šæ„å»ºä¸€ä¸ªæç®€çš„ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰èŠå¤©åº”ç”¨ï¼Œç”¨æˆ·å¯ä»¥å°† TXT æ–‡ä»¶æ”¾å…¥æŒ‡å®šç›®å½•ï¼Œè¿è¡Œè„šæœ¬åé€šè¿‡ç½‘é¡µç•Œé¢ä¸æ–‡æ¡£å†…å®¹è¿›è¡Œå¯¹è¯ã€‚

**æ ¸å¿ƒåŠŸèƒ½**ï¼š

- ğŸ“ æ–‡æ¡£å¤„ç†ï¼šè‡ªåŠ¨ç´¢å¼•æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ TXT æ–‡ä»¶
- ğŸ” æ··åˆæ£€ç´¢ï¼šBM25 å…³é”®è¯æ£€ç´¢ + å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
- ğŸ’¬ æ™ºèƒ½é—®ç­”ï¼šåŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç”Ÿæˆå›ç­”
- ğŸŒ Web ç•Œé¢ï¼šç®€æ´çš„å•é¡µé¢èŠå¤©ç•Œé¢
- ğŸš€ ä¸€é”®éƒ¨ç½²ï¼šè¿è¡Œè„šæœ¬å³å¯å¯åŠ¨æœåŠ¡

**æŠ€æœ¯æ ˆ**ï¼š

- åç«¯ï¼šPython 3.11 + FastAPI + LlamaIndex + ChromaDB
- å‰ç«¯ï¼šHTML + TailwindCSS + Vanilla JavaScript
- LLMï¼šGPT-4o-miniï¼ˆé€šè¿‡ä»£ç† APIï¼‰
- å‘é‡æ¨¡å‹ï¼štext-embedding-3-small

## å¼€å‘æ­¥éª¤

### ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡

#### 1.1 åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„

```bash
mkdir rag-chat-app
cd rag-chat-app

# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p {backend,frontend,data,storage}
mkdir -p backend/{app,config}
mkdir -p frontend/{static/{css,js},templates}
```

#### 1.2 åˆ›å»º Python è™šæ‹Ÿç¯å¢ƒ

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (Windows)
venv\Scripts\activate

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (Linux/Mac)
source venv/bin/activate
```

#### 1.3 å®‰è£…ä¾èµ–åŒ…

åˆ›å»º `requirements.txt`ï¼š

```txt
fastapi==0.115.13
uvicorn[standard]==0.34.3
llama-index==0.12.42
llama-index-vector-stores-chroma
llama-index-embeddings-openai
llama-index-llms-openai
chromadb==1.0.12
pydantic-settings==2.9.1
python-multipart
python-dotenv
```

å®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt
```

### ç¬¬äºŒæ­¥ï¼šé…ç½®ç®¡ç†

#### 2.1 åˆ›å»ºç¯å¢ƒé…ç½®æ–‡ä»¶

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```env
# OpenAI APIé…ç½®
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://your-proxy-url.com/v1
OPENAI_MODEL=gpt-4o-mini
EMBEDDING_MODEL=text-embedding-3-small

# åº”ç”¨é…ç½®
APP_HOST=0.0.0.0
APP_PORT=8000
DATA_DIR=./data
STORAGE_DIR=./storage
COLLECTION_NAME=documents

# CORSé…ç½®
ALLOWED_ORIGINS=["http://localhost:3000", "http://127.0.0.1:3000", "https://chat.example.org"]
```

#### 2.2 åˆ›å»ºé…ç½®ç±»

åˆ›å»º `backend/config/settings.py`ï¼š

```python
from pydantic_settings import BaseSettings
from typing import List
import os

class Settings(BaseSettings):
    # OpenAIé…ç½®
    openai_api_key: str
    openai_base_url: str = "https://api.openai.com/v1"
    openai_model: str = "gpt-4o-mini"
    embedding_model: str = "text-embedding-3-small"

    # åº”ç”¨é…ç½®
    app_host: str = "0.0.0.0"
    app_port: int = 8000
    data_dir: str = "./data"
    storage_dir: str = "./storage"
    collection_name: str = "documents"

    # CORSé…ç½®
    allowed_origins: List[str] = [
        "http://localhost:3000",
        "http://127.0.0.1:3000"
    ]

    class Config:
        env_file = ".env"
        case_sensitive = False

# å…¨å±€é…ç½®å®ä¾‹
settings = Settings()
```

### ç¬¬ä¸‰æ­¥ï¼šåç«¯å¼€å‘

#### 3.1 åˆ›å»º RAG æœåŠ¡æ ¸å¿ƒç±»

åˆ›å»º `backend/app/rag_service.py`ï¼š

```python
import os
import logging
from typing import List, Optional
from pathlib import Path

import chromadb
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    Settings as LlamaSettings
)
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.core.node_parser import SentenceSplitter

from config.settings import settings

logger = logging.getLogger(__name__)

class RAGService:
    def __init__(self):
        self.index = None
        self.query_engine = None
        self._setup_llama_index()
        self._initialize_vector_store()

    def _setup_llama_index(self):
        """é…ç½®LlamaIndexå…¨å±€è®¾ç½®"""
        # é…ç½®LLM
        llm = OpenAI(
            model=settings.openai_model,
            api_key=settings.openai_api_key,
            base_url=settings.openai_base_url,
            temperature=0.1
        )

        # é…ç½®åµŒå…¥æ¨¡å‹
        embed_model = OpenAIEmbedding(
            model=settings.embedding_model,
            api_key=settings.openai_api_key,
            base_url=settings.openai_base_url
        )

        # è®¾ç½®å…¨å±€é…ç½®
        LlamaSettings.llm = llm
        LlamaSettings.embed_model = embed_model
        LlamaSettings.node_parser = SentenceSplitter(
            chunk_size=1024,
            chunk_overlap=200
        )

    def _initialize_vector_store(self):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        try:
            # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
            os.makedirs(settings.storage_dir, exist_ok=True)

            # åˆå§‹åŒ–ChromaDB
            chroma_client = chromadb.PersistentClient(
                path=settings.storage_dir
            )

            # è·å–æˆ–åˆ›å»ºé›†åˆ
            try:
                chroma_collection = chroma_client.get_collection(
                    settings.collection_name
                )
                logger.info(f"åŠ è½½ç°æœ‰é›†åˆ: {settings.collection_name}")
            except:
                chroma_collection = chroma_client.create_collection(
                    settings.collection_name
                )
                logger.info(f"åˆ›å»ºæ–°é›†åˆ: {settings.collection_name}")

            # åˆ›å»ºå‘é‡å­˜å‚¨
            vector_store = ChromaVectorStore(
                chroma_collection=chroma_collection
            )
            storage_context = StorageContext.from_defaults(
                vector_store=vector_store
            )

            # å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•
            try:
                self.index = VectorStoreIndex.from_vector_store(
                    vector_store=vector_store,
                    storage_context=storage_context
                )
                logger.info("æˆåŠŸåŠ è½½ç°æœ‰ç´¢å¼•")
            except:
                # å¦‚æœæ²¡æœ‰ç°æœ‰ç´¢å¼•ï¼Œåˆ›å»ºç©ºç´¢å¼•
                self.index = VectorStoreIndex(
                    nodes=[],
                    storage_context=storage_context
                )
                logger.info("åˆ›å»ºæ–°çš„ç©ºç´¢å¼•")

            # åˆ›å»ºæŸ¥è¯¢å¼•æ“
            self.query_engine = self.index.as_query_engine(
                similarity_top_k=5,
                response_mode="compact"
            )

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            raise

    def load_documents(self) -> bool:
        """åŠ è½½æ•°æ®ç›®å½•ä¸­çš„æ–‡æ¡£"""
        try:
            data_path = Path(settings.data_dir)
            if not data_path.exists():
                logger.warning(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_path}")
                return False

            # æ£€æŸ¥æ˜¯å¦æœ‰TXTæ–‡ä»¶
            txt_files = list(data_path.glob("*.txt"))
            if not txt_files:
                logger.warning(f"æ•°æ®ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°TXTæ–‡ä»¶: {data_path}")
                return False

            logger.info(f"æ‰¾åˆ° {len(txt_files)} ä¸ªTXTæ–‡ä»¶")

            # è¯»å–æ–‡æ¡£
            reader = SimpleDirectoryReader(
                input_dir=str(data_path),
                required_exts=[".txt"],
                recursive=True
            )
            documents = reader.load_data()

            if not documents:
                logger.warning("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£")
                return False

            logger.info(f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

            # é‡æ–°æ„å»ºç´¢å¼•
            self.index = VectorStoreIndex.from_documents(
                documents,
                storage_context=self.index.storage_context
            )

            # æ›´æ–°æŸ¥è¯¢å¼•æ“
            self.query_engine = self.index.as_query_engine(
                similarity_top_k=5,
                response_mode="compact"
            )

            logger.info("æ–‡æ¡£ç´¢å¼•æ„å»ºå®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
            return False

    def query(self, question: str) -> dict:
        """æŸ¥è¯¢RAGç³»ç»Ÿ"""
        try:
            if not self.query_engine:
                return {
                    "success": False,
                    "error": "RAGç³»ç»Ÿæœªåˆå§‹åŒ–",
                    "answer": ""
                }

            logger.info(f"å¤„ç†æŸ¥è¯¢: {question}")

            # æ‰§è¡ŒæŸ¥è¯¢
            response = self.query_engine.query(question)

            # æå–æºæ–‡æ¡£ä¿¡æ¯
            source_nodes = getattr(response, 'source_nodes', [])
            sources = []
            for node in source_nodes:
                if hasattr(node, 'metadata') and 'file_name' in node.metadata:
                    sources.append({
                        "file": node.metadata['file_name'],
                        "score": getattr(node, 'score', 0.0)
                    })

            return {
                "success": True,
                "answer": str(response),
                "sources": sources,
                "error": ""
            }

        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "answer": ""
            }

    def get_status(self) -> dict:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        try:
            data_path = Path(settings.data_dir)
            txt_files = list(data_path.glob("*.txt")) if data_path.exists() else []

            return {
                "initialized": self.query_engine is not None,
                "data_dir_exists": data_path.exists(),
                "txt_files_count": len(txt_files),
                "txt_files": [f.name for f in txt_files]
            }
        except Exception as e:
            logger.error(f"è·å–çŠ¶æ€å¤±è´¥: {e}")
            return {
                "initialized": False,
                "error": str(e)
            }

# å…¨å±€RAGæœåŠ¡å®ä¾‹
rag_service = RAGService()
```
